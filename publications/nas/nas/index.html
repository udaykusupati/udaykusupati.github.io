<!DOCTYPE html>
<html lang="en">
  <head>
  <title>Normal Assisted Stereo Depth Estimation | Uday Kusupati

  </title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-P7FK6B2MG1"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-P7FK6B2MG1');
        }
      </script>
    
  




<meta name="keywords" content="Uday Kusupati">
<meta property="og:locale" content='en)_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="Normal Assisted Stereo Depth Estimation">
<meta property="og:description" content="Normal Assisted Stereo Depth Estimation IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020 Uday Kusupati The University of Texas at Austin">
<meta property="og:url" content="/publications/nas/nas/">
<meta property="og:image" content="images/%!s(&lt;nil&gt;)">
<link rel="canonical" href="/publications/nas/nas/"><link rel="apple-touch-icon" sizes="180x180" href='images/icons/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='images/icons/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='images/icons/favicon-16x16.png'>
<link rel="manifest" href='/images/icons/site.webmanifest'>




<link rel="stylesheet" href="/css/styles.32dbc426c7c10f2a14e9b2e6b08e88ed8bc40a3cb3ee15104a5a8399d200b837f79cc0feec89de39ad26fe0d5d6027131bcd1994dbb0ba538c254824e3543188.css" integrity="sha512-MtvEJsfBDyoU6bLmsI6I7YvECjyz7hUQSlqDmdIAuDf3nMD&#43;7IneOa0m/g1dYCcTG80ZlNuwulOMJUgk41QxiA==" crossorigin="anonymous">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Fira+Code&display=swap" rel="stylesheet">
</head>

  <body>
    <main>
      <div class="nav-drop">
  <div class="nav-body">
      <a href="/" class="nav_item">Home</a>
      <a href="/" class="nav_item">Resume</a>
    <div class="nav-close"></div>
  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href="/" class="nav-brand">Uday Kusupati</a>
    <ul class="nav-labels">
      <li>
        <a href="/" class="nav_item">Home</a>
      </li>
      <li>
        <a href="/" class="nav_item">Resume</a>
      </li>
    </ul>
    <p class="nav-byline pale"></p>
    
      
    </div>
  </nav>
</header>


      
  <article class="post">
    <div>
      <div class="post_body" style="width: 100%;">
        <div class="post_inner">
          <p>




 













<h1>Normal Assisted Stereo Depth Estimation</h1>


<h5 style="margin-top: 0;"><a href=ZgotmplZ></a>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020</a></h5>


<div class="authors header-font">
	
		
		
		
		
		
		
		

		<div class="author-info">

			<span style="text-transform:uppercase"><a class="author-name" href=https://udaykusupati.github.io/>Uday Kusupati</a></span>

			<div class="author-affliation">
			
				
				

				<a class="author-affiliation-name" href="https://www.cs.utexas.edu/">The University of Texas at Austin</a>
			
			</div>

		</div>
	
		
		
		
		
		
		
		

		<div class="author-info">

			<span style="text-transform:uppercase"><a class="author-name" href=https://sites.google.com/view/shuocheng>Shuo Cheng</a></span>

			<div class="author-affliation">
			
				
				

				<a class="author-affiliation-name" href="https://cse.ucsd.edu/">University of California San Diego</a>
			
			</div>

		</div>
	
		
		
		
		
		
		
		

		<div class="author-info">

			<span style="text-transform:uppercase"><a class="author-name" href=https://cray695.wixsite.com/mysite>Rui Chen</a></span>

			<div class="author-affliation">
			
				
				

				<a class="author-affiliation-name" href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a>
			
			</div>

		</div>
	
		
		
		
		
		
		
		

		<div class="author-info">

			<span style="text-transform:uppercase"><a class="author-name" href=https://cseweb.ucsd.edu/~haosu/>Hao Su</a></span>

			<div class="author-affliation">
			
				
				

				<a class="author-affiliation-name" href="https://cse.ucsd.edu/">University of California San Diego</a>
			
			</div>

		</div>
	
	<div class="author-notes pale">
	
	</div>
</div>


</p>
<figure><img src="/publications/nas/nas-crop.gif"
    alt="Illustration of results of separate learning and joint learning of depth and normal. While the normal prediction is smooth and accurate, existing state-of-the-art stereo depth prediction result is noisy. Our method improves the prediction quality significantly by joint learning of depth and normal and enforcing consistency"><figcaption>
      <p>Illustration of results of separate learning and joint learning of depth and normal. While the normal prediction is smooth and accurate, existing state-of-the-art stereo depth prediction result is noisy. Our method improves the prediction quality significantly by joint learning of depth and normal and enforcing consistency</p>
    </figcaption>
</figure>

<h2 id="abstract">Abstract</h2>
<p>Accurate stereo depth estimation plays a critical role in various 3D tasks in both indoor and outdoor environments. Recently, learning-based multi-view stereo methods have demonstrated competitive performance with limited number of views. However, in challenging scenarios, especially when building cross-view correspondences is hard, these methods still cannot produce satisfying results. In this paper, we study how to enforce the consistency between surface normal and depth at training time to improve the performance. We couple the learning of a multi-view normal estimation module and a multi-view depth estimation module. In addition, we propose a novel consistency loss to train an independent consistency module that refines the depths from depth/normal pairs. We find that the joint learning can improve both the prediction of normal and depth, and the accuracy and smoothness can be further improved by enforcing the consistency. Experiments on MVS, SUN3D, RGBD and Scenes11 demonstrate the effectiveness of our method and state-of-the-art performance.</p>
<h2 id="fast-forward">Fast Forward</h2>

<div class="video">
  <iframe src="https://www.youtube.com/embed/F1qrukMlXnY?controls=1&rel=0" loading="lazy"></iframe>
</div>

<h2 id="resources">Resources</h2>
<p><a href="https://arxiv.org/pdf/1911.10444.pdf">Preprint</a></p>
<p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kusupati_Normal_Assisted_Stereo_Depth_Estimation_CVPR_2020_paper.pdf">Publisher Page</a></p>
<p><a href="https://github.com/udaykusupati/Normal-Assisted-Stereo">Source Code</a></p>
<h2 id="cite-us">Cite Us</h2>
<pre tabindex="0"><code>@InProceedings{Kusupati_2020_CVPR,
  author = {Kusupati, Uday and Cheng, Shuo and Chen, Rui and Su, Hao},
  title = {Normal Assisted Stereo Depth Estimation},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2020}
}
</code></pre>
        </div>
      </div>
    </div>
  </article>

    </main>
    <footer class="footer pale header-font">
  <p class="attribution upcase">Uday Kusupati, 2024</p>
  <p class="attribution upcase">Designed By <a href="https://desmondlzy.me">Desmond</a></p>
</footer>
    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
